# jemdoc: menu{MENU}{Research.html}, analytics{UA-63555941-1}
= My Research

== On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima
Joint work with Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy and Ping Tak Peter Tang.

Paper: [https://openreview.net/forum?id=H1oyRlYgg&noteId=H1oyRlYgg ICLR 2017], [https://arxiv.org/abs/1609.04836 arXiv preprint]; Code: [https://github.com/keskarnitish/large-batch-training GitHub]; Poster: [iclr-poster.pdf ICLR 2017 Poster]
~~~
{Abstract}
The stochastic gradient descent method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, usually 32-512 data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a significant degradation in the quality of the model, as measured by its ability to generalize. There have been some attempts to investigate the cause for this generalization drop in the large-batch regime, however the precise answer for this phenomenon is, hitherto unknown. In this paper, we present ample numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions -- and that sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We also discuss several empirical strategies that help large-batch methods eliminate the generalization gap and conclude with a set of future research ideas and open questions.
~~~

== A Limited-Memory Quasi-Newton Algorithm for Bound-Constrained Nonsmooth Optimization
Joint work with Andreas Waechter

Paper: [https://arxiv.org/abs/1612.07350 arXiv preprint]; Code: [https://github.com/keskarnitish/NQN GitHub]
~~~
{Abstract}
We consider the problem of minimizing a continuous function that may be nonsmooth and nonconvex, subject to bound constraints. We propose an algorithm that uses the L-BFGS quasi-Newton approximation of the problem's curvature together with a variant of the weak Wolfe line search. The key ingredient of the method is an active-set selection strategy that defines the subspace in which search directions are computed. To overcome the inherent shortsightedness of the gradient for a nonsmooth function, we propose two strategies. The first relies on an approximation of the Ïµ-minimum norm subgradient, and the second uses an iterative corrective loop that augments the active set based on the resulting search directions. We describe a Python implementation of the proposed algorithm and present numerical results on a set of standard test problems to illustrate the efficacy of our approach.
~~~


== adaQN: An Adaptive Quasi-Newton Algorithm for Training RNNs
Joint work with Albert Berahas. 

Paper: [http://link.springer.com/chapter/10.1007/978-3-319-46128-1_1 Proceedings of ECML-PKDD 2016], [https://arxiv.org/abs/1511.01169 arXiv preprint]; Code: [https://github.com/keskarnitish/minSQN GitHub]
~~~
{Abstract}
Recurrent Neural Networks (RNNs) are powerful models that achieve exceptional performance on several pattern recognition problems. However, the training of RNNs is a computationally difficult task owing to the well-known "vanishing/exploding" gradient problem. Algorithms proposed for training RNNs either exploit no (or limited) curvature information and have cheap per-iteration complexity, or attempt to gain significant curvature information at the cost of increased per-iteration cost. The former set includes diagonally-scaled first-order methods such as ADAGRAD and ADAM, while the latter consists of second-order algorithms like Hessian-Free Newton and K-FAC. In this paper, we present adaQN, a stochastic quasi-Newton algorithm for training RNNs. Our approach retains a low per-iteration cost while allowing for non-diagonal scaling through a stochastic L-BFGS updating scheme. The method uses a novel L-BFGS scaling initialization scheme and is judicious in storing and retaining L-BFGS curvature pairs. We present numerical experiments on two language modeling tasks and show that adaQN is competitive with popular RNN training algorithms.
~~~

== A Second-Order Method for Convex L1-Regularized Optimization with Active Set Prediction
Joint work with Jorge Nocedal, Figen Oztoprak and Andreas Waechter.

Paper: [http://www.tandfonline.com/doi/pdf/10.1080/10556788.2016.1138222 Optimization Methods & Software], [http://arxiv.org/abs/1505.04315 arXiv preprint]; Code: [https://github.com/keskarnitish/oba GitHub] 
~~~
{Abstract}
We describe an active-set method for the minimization of an objective function that is the sum of a smooth convex function and an L1-regularization term. A distinctive feature of the method is the way in which active-set identification and {second-order} subspace minimization steps are integrated to combine the predictive power of the two approaches. At every iteration, the algorithm selects a candidate set of free and fixed variables, performs an (inexact) subspace phase, and then assesses the quality of the new active set. If it is not judged to be acceptable, then the set of free variables is restricted and a new active-set prediction is made.
We establish global convergence for our approach, and compare the new method against the state-of-the-art code LIBLINEAR.
~~~

== A Nonmonotone Learning Rate Strategy for SGD Training of Deep Neural Networks
Joint work with George Saon.

Paper: [http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7178917 Proceedings of IEEE ICASSP (2015)]
~~~
{Abstract}
The algorithm of choice for cross-entropy training of deep neural
network (DNN) acoustic models is mini-batch stochastic gradient
descent (SGD). One of the important decisions for this algorithm is
the learning rate strategy (also called stepsize selection). We
investigate several existing schemes and propose a new learning rate
strategy which is inspired by nonmonotone linesearch techniques in
nonlinear optimization and the NewBob algorithm. This strategy was
found to be relatively insensitive to poorly tuned parameters and
resulted in lower word error rates compared to Newbob on two different
LVCSR tasks (English broadcast news transcription 50 hours and
Switchboard telephone conversations 300 hours). Further, we discuss
some justifications for the method by briefly linking it to results in
optimization theory.
~~~
