# jemdoc: analytics{UA-109844248-1}
= Nitish Shirish Keskar

~~~
{}{img_left}{nitishkeskar_portrait.jpg}{Portrait of Nitish Keskar}{200}{200}
I'm a Senior Research Scientist at Salesforce Research in Palo Alto where I work on Deep Learning and its applications to Natural Language Processing and Computer Vision. I am particularly interested in efficient training methods and issues pertaining to generalization and scalability.

I received my PhD from [http://www.northwestern.edu/ Northwestern University] in 2017 under the supervision of [http://users.iems.northwestern.edu/~nocedal/ Prof. Jorge Nocedal] and [http://users.iems.northwestern.edu/~andreasw/ Prof. Andreas Waechter]. For my PhD, I focused on efficiently finding solutions to Mathematical Optimization problems which are nonsmooth or stochastic. This includes several problems in Machine Learning and Deep Learning. 

# News: 
# - I'll be speaking at the [https://supercomputersfordl2017.github.io/ Deep Learning at Supercomputing Scale] symposium at NIPS 2017. You can find the slides [nips17.key here].  
~~~

== Contact

Email: [keskar.nitish@gmail.com keskar.nitish@gmail.com] \n
Phone: +1-805-312-8841

== Links
- [https://scholar.google.com/citations?user=CJ-_cEEAAAAJ&hl=en Google Scholar]
- [https://semanticscholar.org/author/Nitish-Shirish-Keskar/2924281 Semantic Scholar]
- [https://www.linkedin.com/in/nitishkeskar LinkedIn]
- [https://www.twitter.com/strongduality  Twitter]
- [https://www.github.com/keskarnitish GitHub]

== Publications

- *Identifying Generalization Properties in Neural Networks.* H. Wang, N. Keskar, C. Xiong & R. Socher \n
Paper: [https://arxiv.org/abs/1809.07402 arXiv preprint], [https://einstein.ai/research/blog/identifying-generalization-properties-in-neural-networks blog post]

- *The Natural Language Decathlon: Multitask Learning as Question Answering.* B. McCann, N. Keskar, C. Xiong & R. Socher \n
Paper: [https://einstein.ai/static/images/pages/research/decaNLP/decaNLP.pdf preprint], [https://einstein.ai/research/the-natural-language-decathlon  blog post] \n
Code: [https://github.com/salesforce/decaNLP GitHub] \n
Press: [https://venturebeat.com/2018/06/20/salesforce-develops-natural-language-processing-model-that-performs-10-tasks-at-once/ VentureBeat], [https://www.zdnet.com/article/salesforce-research-creates-swiss-army-knife-for-natural-language-processing/ ZDNet], [http://www.faz.net/aktuell/wirtschaft/kuenstliche-intelligenz/richard-socher-wenn-der-computer-multitasking-kann-15650122.html FAZ (German)], [https://siliconangle.com/blog/2018/06/20/salesforce-claims-big-advance-natural-language-processing/ SiliconAngle]

- *Using Mode Connectivity for Loss Landscape Analysis.* A. Gotmare, N. Keskar, C. Xiong & R. Socher \n
Paper: [https://arxiv.org/abs/1806.06977 arXiv preprint]

- *An Analysis of Neural Language Modeling at Multiple Scales.* S. Merity, N. Keskar & R. Socher \n
Paper: [https://arxiv.org/abs/1803.08240 arXiv preprint] \n Code: [https://github.com/salesforce/awd-lstm-lm GitHub]

- *Regularizing and optimizing LSTM language models.* S. Merity, N. Keskar & R. Socher. \n 
Paper: [https://openreview.net/pdf?id=SyyGPP0TZ ICLR 2018], [https://arxiv.org/abs/1708.02182 arXiv preprint]\n Code: [https://github.com/salesforce/awd-lstm-lm GitHub]

- *Scalable Language Modeling: WikiText-103 on a Single GPU in 12 hours.* S.Merity, N.Keskar, J. Bradbury & R. Socher \n
Paper: [http://www.sysml.cc/doc/50.pdf SysML 2018 (PDF)]

- *Improving Generalization Performance by Switching from Adam to SGD.* N. Keskar & R.Socher \n
Paper: [https://arxiv.org/abs/1712.07628 arXiv preprint]

- *Weighted Transformer Network for Machine Translation.* K. Ahmed, N. Keskar & R. Socher \n
Paper: [https://arxiv.org/abs/1711.02132 arXiv preprint]

- *Balancing Communication and Computation in Distributed Optimization.* A. Berahas, R. Bollapragada, N. Keskar & E. Wei \n
Paper: [https://arxiv.org/abs/1709.02999 arXiv preprint]

- *On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima.* N. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy & P. T. P. Tang  \n
Paper: [https://openreview.net/forum?id=H1oyRlYgg&noteId=H1oyRlYgg ICLR 2017], [https://arxiv.org/abs/1609.04836 arXiv preprint]\n Code: [https://github.com/keskarnitish/large-batch-training GitHub] \n
This paper was selected for an oral presentation at ICLR 2017; only 15 such papers were selected. 

- *A Limited-Memory Quasi-Newton Algorithm for Bound-Constrained Nonsmooth Optimization.* N. Keskar & A. Waechter \n
Paper: [http://www.tandfonline.com/doi/abs/10.1080/10556788.2017.1378652 Optimization Methods & Software], [https://arxiv.org/abs/1612.07350 arXiv preprint]\n Code: [https://github.com/keskarnitish/NQN GitHub]

- *adaQN: An Adaptive Quasi-Newton Algorithm for Training RNNs.* N. Keskar & A. Berahas \n
Paper: [http://link.springer.com/chapter/10.1007/978-3-319-46128-1_1 Proceedings of ECML-PKDD 2016], [https://arxiv.org/abs/1511.01169 arXiv preprint]\n Code: [https://github.com/keskarnitish/minSQN GitHub]

- *A Second-Order Method for Convex L1-Regularized Optimization with Active Set Prediction.* N. Keskar, J. Nocedal, F. Oztoprak & A. Waechter \n
Paper: [http://www.tandfonline.com/doi/pdf/10.1080/10556788.2016.1138222 Optimization Methods & Software], [http://arxiv.org/abs/1505.04315 arXiv preprint]\n Code: [https://github.com/keskarnitish/oba GitHub] \n 
This paper won the [http://explore.tandfonline.com/page/est/charles-broyden-prize 2016 Charles Broyden Prize] for best paper published in the Optimization Methods & Software journal.

- *A Nonmonotone Learning Rate Strategy for SGD Training of Deep Neural Networks.* N. Keskar & G. Saon \n
Paper: [http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7178917 Proceedings of IEEE ICASSP (2015)]






== Research Interests
- Nonlinear Optimization
- Deep Learning
- Machine Learning
- Natural Language Processing
- Scientific Computing
