<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Nitish Shirish Keskar</title>
</head>
<body>
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
    var pageTracker = _gat._getTracker("UA-109844248-1");
    pageTracker._trackPageview();
} catch(err) {}</script>
<div id="layout-content">
<div id="toptitle">
<h1>Nitish Shirish Keskar</h1>
</div>
<table class="imgtable"><tr><td>
<img src="nitishkeskar_portrait.jpg" alt="Portrait of Nitish Keskar" width="140px" height="200px" />&nbsp;</td>
<td align="left"><p>I'm a Senior Research Scientist at Salesforce Research in Palo Alto where I work on Deep Learning and its applications to Natural Language Processing and Computer Vision. I am particularly interested in efficient training methods and issues pertaining to generalization and scalability.</p>
<p>I received my PhD from <a href="http://www.northwestern.edu/">Northwestern University</a> in 2017 under the supervision of <a href="http://users.iems.northwestern.edu/~nocedal/">Prof. Jorge Nocedal</a> and <a href="http://users.iems.northwestern.edu/~andreasw/">Prof. Andreas Waechter</a>. For my PhD, I focused on efficiently finding solutions to Mathematical Optimization problems which are nonsmooth or stochastic. This includes several problems in Machine Learning and Deep Learning. </p>
</td></tr></table>
<h2>Contact</h2>
<p>Email: <a href="mailto:keskar.nitish@gmail.com">keskar.nitish@gmail.com</a> <br />
Phone: +1-805-312-8841</p>
<h2>Links</h2>
<ul>
<li><p><a href="https://scholar.google.com/citations?user=CJ-_cEEAAAAJ&amp;hl=en">Google Scholar</a></p>
</li>
<li><p><a href="https://semanticscholar.org/author/Nitish-Shirish-Keskar/2924281">Semantic Scholar</a></p>
</li>
<li><p><a href="https://www.linkedin.com/in/nitishkeskar">LinkedIn</a></p>
</li>
<li><p><a href="https://www.twitter.com/strongduality">Twitter</a></p>
</li>
<li><p><a href="https://www.github.com/keskarnitish">GitHub</a></p>
</li>
</ul>
<h2>Publications</h2>
<ul>
<li><p><b>The Natural Language Decathlon: Multitask Learning as Question Answering.</b> B. McCann, N. Keskar, C. Xiong &amp; R. Socher <br />
Paper: <a href="https://einstein.ai/static/images/pages/research/decaNLP/decaNLP.pdf">preprint</a>, <a href="https://einstein.ai/research/the-natural-language-decathlon">blog post</a> <br />
Code: <a href="https://github.com/salesforce/decaNLP">GitHub</a> <br />
Press: <a href="https://venturebeat.com/2018/06/20/salesforce-develops-natural-language-processing-model-that-performs-10-tasks-at-once/">VentureBeat</a>, <a href="https://www.zdnet.com/article/salesforce-research-creates-swiss-army-knife-for-natural-language-processing/">ZDNet</a>, <a href="http://www.faz.net/aktuell/wirtschaft/kuenstliche-intelligenz/richard-socher-wenn-der-computer-multitasking-kann-15650122.html">FAZ (German)</a>, <a href="https://siliconangle.com/blog/2018/06/20/salesforce-claims-big-advance-natural-language-processing/">SiliconAngle</a></p>
</li>
</ul>
<ul>
<li><p><b>Using Mode Connectivity for Loss Landscape Analysis.</b> A. Gotmare, N. Keskar, C. Xiong &amp; R. Socher <br />
Paper: <a href="https://arxiv.org/abs/1806.06977">arXiv preprint</a></p>
</li>
</ul>
<ul>
<li><p><b>An Analysis of Neural Language Modeling at Multiple Scales.</b> S. Merity, N. Keskar &amp; R. Socher <br />
Paper: <a href="https://arxiv.org/abs/1803.08240">arXiv preprint</a> <br /> Code: <a href="https://github.com/salesforce/awd-lstm-lm">GitHub</a></p>
</li>
</ul>
<ul>
<li><p><b>Regularizing and optimizing LSTM language models.</b> S. Merity, N. Keskar &amp; R. Socher. <br /> 
Paper: <a href="https://openreview.net/pdf?id=SyyGPP0TZ">ICLR 2018</a>, <a href="https://arxiv.org/abs/1708.02182">arXiv preprint</a><br /> Code: <a href="https://github.com/salesforce/awd-lstm-lm">GitHub</a></p>
</li>
</ul>
<ul>
<li><p><b>Scalable Language Modeling: WikiText-103 on a Single GPU in 12 hours.</b> S.Merity, N.Keskar, J. Bradbury &amp; R. Socher <br />
Paper: <a href="http://www.sysml.cc/doc/50.pdf">SysML 2018 (PDF)</a></p>
</li>
</ul>
<ul>
<li><p><b>Improving Generalization Performance by Switching from Adam to SGD.</b> N. Keskar &amp; R.Socher <br />
Paper: <a href="https://arxiv.org/abs/1712.07628">arXiv preprint</a></p>
</li>
</ul>
<ul>
<li><p><b>Weighted Transformer Network for Machine Translation.</b> K. Ahmed, N. Keskar &amp; R. Socher <br />
Paper: <a href="https://arxiv.org/abs/1711.02132">arXiv preprint</a></p>
</li>
</ul>
<ul>
<li><p><b>Balancing Communication and Computation in Distributed Optimization.</b> A. Berahas, R. Bollapragada, N. Keskar &amp; E. Wei <br />
Paper: <a href="https://arxiv.org/abs/1709.02999">arXiv preprint</a></p>
</li>
</ul>
<ul>
<li><p><b>On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima.</b> N. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy &amp; P. T. P. Tang  <br />
Paper: <a href="https://openreview.net/forum?id=H1oyRlYgg&amp;noteId=H1oyRlYgg">ICLR 2017</a>, <a href="https://arxiv.org/abs/1609.04836">arXiv preprint</a><br /> Code: <a href="https://github.com/keskarnitish/large-batch-training">GitHub</a> <br />
This paper was selected for an oral presentation at ICLR 2017; only 15 such papers were selected. </p>
</li>
</ul>
<ul>
<li><p><b>A Limited-Memory Quasi-Newton Algorithm for Bound-Constrained Nonsmooth Optimization.</b> N. Keskar &amp; A. Waechter <br />
Paper: <a href="http://www.tandfonline.com/doi/abs/10.1080/10556788.2017.1378652">Optimization Methods &amp; Software</a>, <a href="https://arxiv.org/abs/1612.07350">arXiv preprint</a><br /> Code: <a href="https://github.com/keskarnitish/NQN">GitHub</a></p>
</li>
</ul>
<ul>
<li><p><b>adaQN: An Adaptive Quasi-Newton Algorithm for Training RNNs.</b> N. Keskar &amp; A. Berahas <br />
Paper: <a href="http://link.springer.com/chapter/10.1007/978-3-319-46128-1_1">Proceedings of ECML-PKDD 2016</a>, <a href="https://arxiv.org/abs/1511.01169">arXiv preprint</a><br /> Code: <a href="https://github.com/keskarnitish/minSQN">GitHub</a></p>
</li>
</ul>
<ul>
<li><p><b>A Second-Order Method for Convex L1-Regularized Optimization with Active Set Prediction.</b> N. Keskar, J. Nocedal, F. Oztoprak &amp; A. Waechter <br />
Paper: <a href="http://www.tandfonline.com/doi/pdf/10.1080/10556788.2016.1138222">Optimization Methods &amp; Software</a>, <a href="http://arxiv.org/abs/1505.04315">arXiv preprint</a><br /> Code: <a href="https://github.com/keskarnitish/oba">GitHub</a> <br /> 
This paper won the <a href="http://explore.tandfonline.com/page/est/charles-broyden-prize">2016 Charles Broyden Prize</a> for best paper published in the Optimization Methods &amp; Software journal.</p>
</li>
</ul>
<ul>
<li><p><b>A Nonmonotone Learning Rate Strategy for SGD Training of Deep Neural Networks.</b> N. Keskar &amp; G. Saon <br />
Paper: <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7178917">Proceedings of IEEE ICASSP (2015)</a></p>
</li>
</ul>
<h2>Research Interests</h2>
<ul>
<li><p>Nonlinear Optimization</p>
</li>
<li><p>Deep Learning</p>
</li>
<li><p>Machine Learning</p>
</li>
<li><p>Natural Language Processing</p>
</li>
<li><p>Scientific Computing</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2018-06-23 13:55:28 PDT, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</div>
</body>
</html>
